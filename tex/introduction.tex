\label{section}
Hidden Markov models (HMMs) are a fundamental tool for data analysis and exploration.  Many variants of the basic HMM have been developed in response to shortcomings in the original HMM formulation \cite{Rabiner89}.  In this paper we address inference in the explicit state duration HMM.  By state duration we mean the amount of time an HMM dwells in a state.  In the standard HMM specification, a state's duration is implicit and, a priori, distributed geometrically.

The explicit state duration HMM (EDHMM) \cite{Rabiner89} was developed to allow explicit parameterization and direct inference of state duration distributions.  Approximate EDHMM estimation and inference can, in some cases, be performed using a modified forward-backward algorithm; specifically if either the sequence is short or a tight ``allowable'' duration interval for each state is hard-coded a priori \cite{Yu2006}.   If the sequence is short then forward-backward can be run on a state representation that allows for all possible durations shorter than the observed sequence length.  If the sequence is long then forward-backward only remains computationally tractable if only transitions between ``allowed'' durations (those that lie within pre-specified allowable intervals) are considered.   If the true state durations lie outside those intervals then the resulting model estimates will be incorrect in the sense that the learned duration distributions will reflect only what is allowed given the pre-specified duration intervals (which may poorly describe the true duration distributions).

Our contribution is the development of a procedure for EDHMM inference that does not require any hard pre-specification of duration intervals and, as a result, is no longer approximate.  The technique we use to do this 
is borrowed from sampling procedures developed for nonparametric  Bayesian HMM variants \cite{vanGael2008}.  Our key insight is simple: the machinery developed for doing inference in HMMs with a countable number of states is precisely the same as that which is needed for doing inference in an EDHMM with duration distributions over countable support.  So, while the EDHMM is a distinctly parametric model, the tools from nonparametric Bayesian inference can be applied such that black-box inference becomes possible and, in fact, efficient.

In this work we show specifically that a ``beam-sampling'' approach  \cite{vanGael2008} works for non-approximate estimation of EDHMMs, learning both the transition structure and duration distributions simultaneously.

In demonstrating our EDHMM inference technique we consider a synthetic system  in which the state-cardinality is known and finite, but where each state's duration distribution is unknown. We show that the EDHMM beam sampler performs accurate tracking whilst capturing the duration distributions as well as the probability of transitioning between states.

The remainder of the letter is organised as follows.  In Section~\ref{sec:Model} we introduce the EDHMM; in Section~\ref{sec:inference} we review beam-sampling for the infinite Hidden Markov Model (iHMM) \cite{Beal2002} and show how it relates to the EDHMM inference problem; and in Section~\ref{sec:experiments} we show results from using the EDHMM to model synthetic data.  
